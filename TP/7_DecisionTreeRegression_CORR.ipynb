{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b3d60e",
   "metadata": {},
   "source": [
    "# Arbres de décision en Python: cas de la régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1838c04",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"files/figures/DT_1.png\" width=\"300px\"/> </td>\n",
    "<td> </td>\n",
    "<td> </td>\n",
    "<td> <img src=\"files/figures/DT_2.png\" width=\"300px\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d670aa3",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80fb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159601ec",
   "metadata": {},
   "source": [
    "## Loader les data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9c1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/airfoil_self_noise.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa869341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>126.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x0   x1      x2    x3        x4        y\n",
       "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
       "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
       "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
       "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
       "4  2000  0.0  0.3048  71.3  0.002663  127.461"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e055f",
   "metadata": {},
   "source": [
    "Nous allons prédire la variable `y` en fonction des variables `x0`, `x1`, `x2`, `x3`, `x4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bd645",
   "metadata": {},
   "source": [
    "## Arbre de décision dans un contexte de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a819e",
   "metadata": {},
   "source": [
    "La classe `DecisionNode` ci-dessous représente un **noeud** d'un **arbre de décision**, ou encore, un **arbre de décision** en soi. \n",
    "\n",
    "*En effet, en programmation, un objet \"noeud\" qui pointe vers ses fils est une structure de données suffisante pour représenter un arbre binaire.*\n",
    "\n",
    "- Un **noeud interne** représente un *split* de la forme $x_m \\leq s$ (où $x_m$ est la $m$-ième composante de $\\mathbf{x}$).<br>\n",
    "Chaque noeud interne donc est associé à un `feature_index` $m$ et un `threshold` $s$.<br>\n",
    "Un noeud interne est également associé à `var_red`: la réduction de variance engendrée par son split.\n",
    "\n",
    "\n",
    "- Une **feuille** représente un sous-ensemble des data $R_k$ (*région*, *cellule de partition*).<br>\n",
    "Chaque feuille est donc associée une `value`: la target moyenne des data dans $R_m$<br> \n",
    "\n",
    "$$\\hat{y}_{R_k} = \\frac{1}{|R_k|} \\sum_{\\{i : \\mathbf{x_i} \\in R_k\\}} y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30a9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"\n",
    "    Implements a decision node, or quivalently, a decision tree.\n",
    "    \n",
    "    As usual, a binary tree is identified with a root node contaning as chidren\n",
    "    a left subtree and a right subtree.\n",
    "    \n",
    "    Here, an *internal node* represents a split of the form \"x_m <= s\",\n",
    "    where m is the `feature_index` and s the `thresold` of the split.\n",
    "    \n",
    "    A *leaf node* represents a subset of the data (region, box, partition cell).\n",
    "    It is associated with a `value`: the average target for the data in this region.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_index=None, threshold=None, \n",
    "                 left=None, right=None, var_red=None, value=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_index : int\n",
    "            index m of variable x_m for the node split (x_m <= s).\n",
    "        threshold : Union[int, float]\n",
    "            threshold s for the node split (x_m <= s).\n",
    "        left : Union[DecisionNode, None]\n",
    "            Left child of the node\n",
    "        right : Union[DecisionNode, None]\n",
    "            Right child of the node\n",
    "        var_red : float\n",
    "            variance reduction induced by the node split (x_m <= s).\n",
    "        value : Union[float, None]\n",
    "            if the node is a leaf, then value of this leaf.\n",
    "        \"\"\"\n",
    "\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.var_red = var_red\n",
    "\n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "        \n",
    "        # children\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "        \n",
    "    def print_tree(self, tab=0):\n",
    "        \"\"\"Prints the node (i.e., the tree).\"\"\"\n",
    "        \n",
    "        if self.left is None and self.right is None: # leaf\n",
    "            \n",
    "            print(\"\\t\"*(tab), f\"{self.value:.2f}\")\n",
    "            \n",
    "        else:                                        # internal node\n",
    "            self.left.print_tree(tab+1)\n",
    "            print(\"\\t\"*(tab), f\"X_{self.feature_index} < {self.threshold:.2f}\")\n",
    "            self.right.print_tree(tab+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a5510",
   "metadata": {},
   "source": [
    "La classe ci-dessous `DecisionTreeRegressor` représente un **arbre de décision** dans un contexte de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe70cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    \"\"\"\n",
    "    Implements a decision tree for regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_samples : int\n",
    "            Minimal number of data necessary to envision a split.\n",
    "            Otherwise, the node is a leaf (subset of data).\n",
    "        max_depth : int\n",
    "            Maximal depth of the tree.\n",
    "            When max_depth is reached, no further split is performed.\n",
    "        \"\"\"\n",
    "        \n",
    "        # the root will become the decision tree\n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    \n",
    "    def split_data(self, dataset, split):\n",
    "        \"\"\"\n",
    "        Splits dataset according to a given split of the form x_m <= s.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset to be split\n",
    "        split : tuple\n",
    "            A split \"x_m <= s\" is represented by the tuple (m, s),\n",
    "            where m is the feature_index and s the threshold.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dataset_left, dataset_right : tuple[ndarray, ndarray]\n",
    "            dataset_left: data satisfying the condition x_m <= s.\n",
    "            dataset_right: data satisfying the scondition x_m > s.\n",
    "        \"\"\"\n",
    "        \n",
    "        feature_index, threshold = split\n",
    "        mask = dataset[:, feature_index] <= threshold\n",
    "        dataset_left = dataset[mask, :]\n",
    "        dataset_right = dataset[np.logical_not(mask), :]\n",
    "        \n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    \n",
    "    def variance_reduction(self, dataset, dataset_left, dataset_right):\n",
    "        \"\"\"\n",
    "        Computes the variance reduction of the targets induced by a split.\n",
    "        \n",
    "        Suppose that a split of a dataset has induces the 2 datasets:\n",
    "        dataset_left and dataset_right (cf. method split_data).\n",
    "        This function computes the variance $var_1$ of the y's of dataset as well as\n",
    "        the weighted variance $var_2$ of the y's of left_dataset and right_dataset.\n",
    "        The variance reduction is then given by $var_1 - var_2$.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset before split.\n",
    "        dataset_left : ndarray\n",
    "            First dataset induced by the split.\n",
    "        dataset_right : ndarray\n",
    "            Second dataset induced by the split.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        var_reduction : float\n",
    "            variance reduction induced by the split.\n",
    "        \"\"\"\n",
    "        \n",
    "        y = dataset[:, -1]\n",
    "        y_l = dataset_left[:, -1]\n",
    "        y_r = dataset_right[:, -1]\n",
    "        \n",
    "        w_l = len(y_l) / len(y)\n",
    "        w_r = len(y_r) / len(y)\n",
    "        \n",
    "        var_reduction = np.var(y) - ( w_l * np.var(y_l) + w_r * np.var(y_r) )\n",
    "        \n",
    "        return var_reduction\n",
    "\n",
    "    \n",
    "    def best_split(self, dataset):\n",
    "        \"\"\"\n",
    "        Computes the best split for a dataset.\n",
    "        \n",
    "        For all feature $m$ and and all possible value $s$ of that feature,\n",
    "        split the dataset according to the condition \"x_m <= s\" (self.split_data(...)).\n",
    "        The split \"x_m <= s\" generates two datatsets: dataset_left and dataset_right.\n",
    "        Compute the variance reduction associated to the three datasets (self.variance_reduction).\n",
    "        Select the split associated with the largest variance reduction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset before split.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        best_split : dict\n",
    "            dictionary to store the best split.\n",
    "            The keys of the dictionare are: \n",
    "            \"feature_index\", \"threshold\", \"dataset_left\", \"dataset_right\", \"var_red\".\n",
    "        \"\"\"\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        best_var_red = -float(\"inf\")\n",
    "        \n",
    "        # loop over features\n",
    "        nb_features = dataset.shape[1] - 1\n",
    "        for m in range(nb_features):\n",
    "            \n",
    "            thresholds = dataset[:, m]\n",
    "            thresholds = np.unique(thresholds)\n",
    "            \n",
    "            # loop over thresholds\n",
    "            for s in thresholds:\n",
    "                \n",
    "                # datasets assoociated to split \"x_m <= s\"\n",
    "                split = (m, s)\n",
    "                dataset_left, dataset_right = self.split_data(dataset, split)\n",
    "                \n",
    "                # check if datasets are not empty\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    \n",
    "                    # compute variance reduction\n",
    "                    var_red = self.variance_reduction(dataset, dataset_left, dataset_right)\n",
    "                    \n",
    "                    # update the best split if needed\n",
    "                    if var_red > best_var_red:\n",
    "                        \n",
    "                        best_split[\"feature_index\"] = m\n",
    "                        best_split[\"threshold\"] = s\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"var_reduction\"] = var_red\n",
    "                        best_var_red = var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    \n",
    "    def leaf_value(self, region):\n",
    "        \"\"\"\n",
    "        Compute the mean of targets of a non-splitable region. \n",
    "        Teh region is associated to a leaf node of the decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : ndarray\n",
    "            Non-splitable dataset that corresponds to a region oof the partition.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mean : float\n",
    "            Mean of targets for the data in the region.\n",
    "        \"\"\"\n",
    "        \n",
    "        Y = region[:, -1]\n",
    "        mean = np.mean(Y)\n",
    "        \n",
    "        return mean\n",
    "    \n",
    "    \n",
    "    def build_tree(self, dataset, depth=0):\n",
    "        \"\"\"\n",
    "        Builds the decision tree recursively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Initial dataset to be split by the decision tree.\n",
    "        depth : int\n",
    "            Depth of the decision tree that has been built so far.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DecisionNode : DecisionNode\n",
    "            The decision tree that represent the best successive splits\n",
    "            for to the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_samples = np.shape(dataset)[0]\n",
    "                \n",
    "        # compute decision node\n",
    "        if num_samples >= self.min_samples and depth <= self.max_depth:\n",
    "            \n",
    "            # find the best split\n",
    "            split = self.best_split(dataset)\n",
    "            \n",
    "            # check if variance reduction is positive\n",
    "            if split[\"var_reduction\"] >= 0:\n",
    "                \n",
    "                # recursive call left\n",
    "                subtree_left = self.build_tree(split[\"dataset_left\"], depth + 1)\n",
    "                \n",
    "                # recursive call right\n",
    "                subtree_right = self.build_tree(split[\"dataset_right\"], depth + 1)\n",
    "                \n",
    "                return DecisionNode(split[\"feature_index\"], split[\"threshold\"], \n",
    "                                    subtree_left, subtree_right, split[\"var_reduction\"])\n",
    "            \n",
    "        # compute leaf node\n",
    "        else:\n",
    "            \n",
    "            value = self.leaf_value(dataset)\n",
    "            \n",
    "            return DecisionNode(value=value)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fits the decision tree on the features X and targets Y.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Feature columns of the dataset.\n",
    "        Y : ndarray\n",
    "            Target column of the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "        \n",
    "    def predict_single_point(self, x, tree):\n",
    "        \"\"\"        \n",
    "        Predict the target y_hat associated to a point x.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Point x = (x_1,...,x_M) whose target is to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_hat : float\n",
    "            Target associated to x.\n",
    "        \"\"\"\n",
    "        \n",
    "        if tree.value != None:\n",
    "            \n",
    "            return tree.value\n",
    "        \n",
    "        # x_m <= s\n",
    "        if x[tree.feature_index] <= tree.threshold:\n",
    "            \n",
    "            return self.predict_single_point(x, tree.left)\n",
    "        \n",
    "        # x_m > s\n",
    "        else:\n",
    "            \n",
    "            return self.predict_single_point(x, tree.right)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the targets associated to a set of points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Tensor of points X (dim N x M) whose targets are to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray\n",
    "            Tensor of targets (dim N x 1) associated to X.\n",
    "        \"\"\"\n",
    "        \n",
    "        preditions = np.array([self.predict_single_point(x, self.root) for x in X])\n",
    "        \n",
    "        return preditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76749949",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "Complétez la méthode `split_data(...)` qui, étant donné un dataset `dataset` et un split `split` de la forme $x_m \\leq s$, retourne deux datasets qui correspondent aux lignes de `dataset` qui satisfont et ne satisfont pas la condition `split`, respectivement.\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "split = (2, 0.1) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "dataset_left.shape, dataset_right.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d6346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((515, 6), (988, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "split = (2, 0.1) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "dataset_left.shape, dataset_right.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338986fa",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "Complétez la méthode `variance_reduction(...)` qui, étant donné trois datasets `dataset`, `dataset_left` et `dataset_right`, retourne la **réduction de variance** associée aux targets $y_i$ de `dataset`, `dataset_left` et `dataset_right`.\n",
    "\n",
    "La **réduction de variance** est calculée comme suit:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "var   & = & \\mathrm{Var} \\left\\{ y_i : (\\mathbf{x_i}, y_i) \\in \\mathrm{dataset} \\right\\} \\\\\n",
    "&& \\\\\n",
    "var_l & = & \\frac{|\\mathrm{dataset\\_left}|}{|\\mathrm{dataset}|} \\cdot \\mathrm{Var} \\left\\{ y_i : (\\mathbf{x_i}, y_i) \\in \\mathrm{dataset\\_left} \\right\\} \\\\\n",
    "&& \\\\\n",
    "var_r & = & \\frac{|\\mathrm{dataset\\_right}|}{|\\mathrm{dataset}|} \\cdot \\mathrm{Var} \\left\\{ y_i : (\\mathbf{x_i}, y_i) \\in \\mathrm{dataset\\_right} \\right\\} \\\\\n",
    "&& \\\\\n",
    "reduction & = & var - (var_l + var_r)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "où $|X|$ dénote le nombre d'éléments de l'ensemble $X$.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "split = (2, 0.1) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "tree.variance_reduction(dataset, dataset_left, dataset_right)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4544d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.87241336141809"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "split = (2, 0.1) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "tree.variance_reduction(dataset, dataset_left, dataset_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10830a27",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "\n",
    "Complétez la méthode `best_split(...)` qui, étant donné un dataset `dataset`, retourne le split induisant la plus grande réduction de variance. Ce best split est obtenu par l'algorithme suivant:\n",
    "\n",
    "<img src=\"files/figures/DT_algo_1.png\" width=\"650px\"/>\n",
    "\n",
    "La variable `best_split` sera un dictionnaire avec les clés suivantes:\n",
    "```\n",
    "\"feature_index\"\n",
    "\"threshold\"\n",
    "\"dataset_left\"\n",
    "\"dataset_right\"\n",
    "\"var_reduction\"\n",
    "```\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819b6632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3150.0, 7.64827145164071)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "best_split[\"feature_index\"], best_split[\"threshold\"], best_split[\"var_reduction\"]\n",
    "# best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79d905",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "\n",
    "Complétez la méthode `leaf_value(...)` qui, étant donné un sous-ensemble du dataset `region`, retourne la moyenne des targets de ce sous-dataset. On rappelle que chaque feuille de l'arbre de décision est associée à une région non-splitable du dataset.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "region = best_split['dataset_right']\n",
    "tree.leaf_value(region)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc5a92ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.42420754716981"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "region = best_split['dataset_right']\n",
    "tree.leaf_value(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e378d64",
   "metadata": {},
   "source": [
    "### Exercice 5\n",
    "\n",
    "Complétez la méthode **récursive** `build_tree(...)` qui, étant donné un dataset `dataset` et une profondeur `depth`, retourne l'arbre de décision correspondant à ce dataset. L'arbre de décision est donné par l'algorithme **recursive binary splitting** ci-dessous.\n",
    "\n",
    "<img src=\"files/figures/DT_algo_2.png\" width=\"650px\"/>\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "dataset = np.array(data)\n",
    "tree = DecisionTreeRegressor(min_samples=3, max_depth=3)\n",
    "tree.root = tree.build_tree(dataset)\n",
    "tree.root.print_tree()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe079b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t 126.94\n",
      "\t\t\t X_0 < 630.00\n",
      "\t\t\t\t 130.52\n",
      "\t\t X_2 < 0.10\n",
      "\t\t\t\t 127.91\n",
      "\t\t\t X_0 < 1600.00\n",
      "\t\t\t\t 122.97\n",
      "\t X_4 < 0.02\n",
      "\t\t\t\t 125.68\n",
      "\t\t\t X_0 < 1250.00\n",
      "\t\t\t\t 120.69\n",
      "\t\t X_4 < 0.05\n",
      "\t\t\t\t 117.70\n",
      "\t\t\t X_0 < 1250.00\n",
      "\t\t\t\t 109.80\n",
      " X_0 < 3150.00\n",
      "\t\t\t\t 134.99\n",
      "\t\t\t X_4 < 0.00\n",
      "\t\t\t\t 128.60\n",
      "\t\t X_0 < 6300.00\n",
      "\t\t\t\t 128.05\n",
      "\t\t\t X_4 < 0.00\n",
      "\t\t\t\t 120.91\n",
      "\t X_4 < 0.00\n",
      "\t\t\t\t 120.09\n",
      "\t\t\t X_0 < 5000.00\n",
      "\t\t\t\t 115.53\n",
      "\t\t X_4 < 0.03\n",
      "\t\t\t\t 109.44\n",
      "\t\t\t X_4 < 0.05\n",
      "\t\t\t\t 104.84\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(data)\n",
    "tree = DecisionTreeRegressor(min_samples=3, max_depth=3)\n",
    "tree.root = tree.build_tree(dataset)\n",
    "tree.root.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82f1f7",
   "metadata": {},
   "source": [
    "### Exercice 6\n",
    "\n",
    "Complétez la méthode `fit(...)` qui correspond à l'entaînement de l'arbre de décision sur les features `X` et les targets `Y` du dataset. Cette méthode construit l'arbre de décision associé au dataset $(X, Y)$ et assigne ce dernier comme racine de l'arbre.\n",
    "\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(min_samples=3, max_depth=3)\n",
    "regressor.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "regressor.root.print_tree()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afea3ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t 126.94\n",
      "\t\t\t X_0 < 630.00\n",
      "\t\t\t\t 130.44\n",
      "\t\t X_2 < 0.10\n",
      "\t\t\t\t 128.04\n",
      "\t\t\t X_0 < 1600.00\n",
      "\t\t\t\t 122.99\n",
      "\t X_4 < 0.02\n",
      "\t\t\t\t 125.86\n",
      "\t\t\t X_0 < 1250.00\n",
      "\t\t\t\t 120.98\n",
      "\t\t X_4 < 0.05\n",
      "\t\t\t\t 117.71\n",
      "\t\t\t X_0 < 1250.00\n",
      "\t\t\t\t 110.31\n",
      " X_0 < 3150.00\n",
      "\t\t\t\t 134.15\n",
      "\t\t\t X_4 < 0.00\n",
      "\t\t\t\t 128.46\n",
      "\t\t X_0 < 8000.00\n",
      "\t\t\t\t 128.08\n",
      "\t\t\t X_4 < 0.00\n",
      "\t\t\t\t 122.73\n",
      "\t X_4 < 0.00\n",
      "\t\t\t\t 123.96\n",
      "\t\t\t X_0 < 6300.00\n",
      "\t\t\t\t 117.00\n",
      "\t\t X_4 < 0.00\n",
      "\t\t\t\t 122.34\n",
      "\t\t\t X_2 < 0.03\n",
      "\t\t\t\t 115.09\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(min_samples=3, max_depth=3)\n",
    "regressor.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "regressor.root.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e225980",
   "metadata": {},
   "source": [
    "### Exercice 7\n",
    "\n",
    "Complétez la méthode **récursive** `predict_single_point(...)` qui retourne la prédiction `y_hat` associée au point `x`. Cette méthode fait passer `x` dans les noeuds sucessifs de l'arbre de décision `tree` jusqu'à atteindre une feuille dont la valeur sera la prédiction.\n",
    "\n",
    "Complétez ensuite la méthode `predict(...)` qui retourne l'enemble des prédictions `X` associée à un ensemble de points `X` (tenseur). Cette méthode appelle la précédente sur chaque ligne de `X`.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(min_samples=3, max_depth=10)\n",
    "regressor.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "regressor.predict_single_point(np.array[0.1, 0.2, 0.3, 0.4. 0.5], regressor.root)\n",
    "regressor.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5619cf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(min_samples=3, max_depth=10)\n",
    "regressor.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "regressor.predict_single_point(np.array([0.1, 0.2, 0.3, 0.4, 0.5]), regressor.root)\n",
    "# regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c1adc",
   "metadata": {},
   "source": [
    "### Exercice 8\n",
    "\n",
    "Exécutez le code ci-dessous pour voir si votre implémentation de la classe ``DecisionTreeRegressor`` fonctionne coorrectement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456c3a",
   "metadata": {},
   "source": [
    "#### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b310c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744240d",
   "metadata": {},
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aafa670",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(min_samples=3, max_depth=10)\n",
    "regressor.fit(X_train,Y_train)\n",
    "# regressor.root.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fe091",
   "metadata": {},
   "source": [
    "#### Résultats sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8119214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.5960217473933724\n"
     ]
    }
   ],
   "source": [
    "Y_pred = regressor.predict(X_test)\n",
    "print(\"MSE:\", np.sqrt(mean_squared_error(Y_test, Y_pred)))\n",
    "# print(\"Test predictions:\\n\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0155dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Targets vs Predictions')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"Test targets\")\n",
    "plt.ylabel(\"Test predictions\")\n",
    "plt.title(\"Targets vs Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4b91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
