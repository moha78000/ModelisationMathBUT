{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b3d60e",
   "metadata": {},
   "source": [
    "# Arbres de décision en Python: cas de la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0194ed",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"files/figures/DT_1.png\" width=\"300px\"/> </td>\n",
    "<td> </td>\n",
    "<td> </td>\n",
    "<td> <img src=\"files/figures/DT_2.png\" width=\"300px\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d670aa3",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80fb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159601ec",
   "metadata": {},
   "source": [
    "## Loader les data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9c1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa869341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
       "0             5.1          3.5           1.4          0.2     Setosa\n",
       "1             4.9          3.0           1.4          0.2     Setosa\n",
       "2             4.7          3.2           1.3          0.2     Setosa\n",
       "3             4.6          3.1           1.5          0.2     Setosa\n",
       "4             5.0          3.6           1.4          0.2     Setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  Virginica\n",
       "146           6.3          2.5           5.0          1.9  Virginica\n",
       "147           6.5          3.0           5.2          2.0  Virginica\n",
       "148           6.2          3.4           5.4          2.3  Virginica\n",
       "149           5.9          3.0           5.1          1.8  Virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e055f",
   "metadata": {},
   "source": [
    "Nous allons prédire la variable `variety` en fonction des variables `sepal.length`, \t`sepal.width`, `petal.length`, `petal.width`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bd645",
   "metadata": {},
   "source": [
    "## Arbre de décision dans un contexte de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a819e",
   "metadata": {},
   "source": [
    "La classe `DecisionNode` ci-dessous représente un **noeud** d'un **arbre de décision**, ou encore, un **arbre de décision** en soi. \n",
    "\n",
    "*En effet, en programmation, un objet \"noeud\" qui pointe vers ses fils est une structure de données suffisante pour représenter un arbre binaire.*\n",
    "\n",
    "- Un **noeud interne** représente un *split* de la forme $x_k \\leq s$ (où $x_k$ est la $k$-ième composante de $\\mathbf{x}$).<br>\n",
    "Chaque noeud interne donc est associé à un `feature_index` $k$ et un `threshold` $s$.<br>\n",
    "Un noeud interne est également associé à `gain`: le gain informationnel (défini en terme d'entropie...) engendré par son split.\n",
    "\n",
    "\n",
    "- Une **feuille** représente un sous-ensemble des data $R_m$ (*région*, *cellule de partition*).<br>\n",
    "Chaque feuille est donc associée une `value`: la target qui apparaît le plus souvent dans les data dans $R_m$:<br>\n",
    "\n",
    "$$\\hat{y}_{R_m} = \\arg \\max \\{ \\mathrm{freq}_{R_m}(y_i) : \\mathbf{x_i} \\in R_m \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f30a9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"\n",
    "    Implements a decision node, or quivalently, a decision tree.\n",
    "    \n",
    "    As usual, a binary tree is identified with a root node contaning as chidren\n",
    "    a left subtree and a right subtree.\n",
    "    \n",
    "    Here, an *internal node* represents a split of the form \"x_m <= s\",\n",
    "    where m is the `feature_index` and s the `thresold` of the split.\n",
    "    \n",
    "    A *leaf node* represents a subset of the data (region, box, partition cell).\n",
    "    It is associated with a `value`: the most fequent target of the data in this region.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_index=None, threshold=None, \n",
    "                 left=None, right=None, gain=None, value=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_index : int\n",
    "            index i of variable x_m for the node split (x_m <= s).\n",
    "        threshold : Union[int, float]\n",
    "            threshold s for the node split (x_m <= s).\n",
    "        left : Union[DecisionNode, None]\n",
    "            Left child of the node\n",
    "        right : Union[DecisionNode, None]\n",
    "            Right child of the node\n",
    "        gain : float\n",
    "            information gain induced by the node split (x_m <= s).\n",
    "        value : Union[float, None]\n",
    "            if the node is a leaf, then value of this leaf.\n",
    "        \"\"\"\n",
    "\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.gain = gain\n",
    "\n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "        \n",
    "        # children\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def print_tree(self, tab=0):\n",
    "        \"\"\"Prints the node (i.e., the tree).\"\"\"\n",
    "        \n",
    "        if self.left is None and self.right is None: # leaf\n",
    "            \n",
    "            print(\"\\t\"*(tab), f\"{self.value:}\")\n",
    "            \n",
    "        else:                                        # internal node\n",
    "            self.left.print_tree(tab+1)\n",
    "            print(\"\\t\"*(tab), f\"X_{self.feature_index} < {self.threshold:.2f}\")\n",
    "            self.right.print_tree(tab+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a5510",
   "metadata": {},
   "source": [
    "La classe `DecisionTreeClassifier` représente un **arbre de décision** dans un contexte de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fe70cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    \"\"\"\n",
    "    Implements a decision tree for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_samples : int\n",
    "            Minimal number of data necessary to envision a split.\n",
    "            Otherwise, the node is a leaf (subset of data).\n",
    "        max_depth : int\n",
    "            Maximal depth of the tree.\n",
    "            When max_depth is reached, no further split is performed.\n",
    "        \"\"\"\n",
    "        \n",
    "        # the root will become the decision tree\n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    \n",
    "    def split_data(self, dataset, split):\n",
    "        \"\"\"\n",
    "        Splits dataset according to a given split of the form x_m <= s.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset to be split\n",
    "        split : tuple\n",
    "            A split \"x_m <= s\" is rerpresented by the tuple (m, s),\n",
    "            where m is the feature_index and s the threshold.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dataset_left, dataset_right : tuple[ndarray, ndarray]\n",
    "            dataset_left: data satisfying the condition x_m <= s.\n",
    "            dataset_right: data satisfying the scondition x_m > s.\n",
    "        \"\"\"\n",
    "        \n",
    "        feature_index, threshold = split\n",
    "        mask = dataset[:, feature_index] <= threshold\n",
    "        dataset_left = dataset[mask, :]\n",
    "        dataset_right = dataset[np.logical_not(mask), :]\n",
    "        \n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    \n",
    "    def entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes the entropy of a set of targets y.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray\n",
    "            Target column of the dataset.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        entropy : float\n",
    "            Entropy of the targets y.\n",
    "        \"\"\"\n",
    "        \n",
    "        labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        \n",
    "        for cls in labels:\n",
    "            \n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        \n",
    "        return entropy \n",
    "    \n",
    "    \n",
    "    def information_gain(self, dataset, dataset_left, dataset_right):\n",
    "        \"\"\"\n",
    "        Computes the information gain induced by a split.\n",
    "        Information gain can be measued in two ways:\n",
    "        *Gini index* or *entropy* of the targets.\n",
    "        \n",
    "        Suppose that a split of a dataset has induces the 2 datasets:\n",
    "        dataset_left and dataset_right (cf. method split_data).\n",
    "        This function computes the entropy $ent_1$ of the y's of dataset as well as\n",
    "        the weighted entropy $ent_2$ of the y's of left_dataset and right_dataset.\n",
    "        The information gain is then given by $ent_1 - ent_2$.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset before split.\n",
    "        dataset_left : ndarray\n",
    "            First dataset induced by the split.\n",
    "        dataset_right : ndarray\n",
    "            Second dataset induced by the split.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gain : float\n",
    "            information gain (i.e., entropy reduction) induced by the split.\n",
    "        \"\"\"\n",
    "        \n",
    "        y = dataset[:, -1]\n",
    "        y_l = dataset_left[:, -1]\n",
    "        y_r = dataset_right[:, -1]\n",
    "        \n",
    "        w_l = len(y_l) / len(y)\n",
    "        w_r = len(y_r) / len(y)\n",
    "        \n",
    "        gain = self.entropy(y) - ( w_l * self.entropy(y_l) + w_r * self.entropy(y_r) )\n",
    "        \n",
    "        return gain\n",
    "\n",
    "    \n",
    "    def best_split(self, dataset):\n",
    "        \"\"\"\n",
    "        Computes the best split for a dataset.\n",
    "        \n",
    "        For all feature $m$ and and all possible value $s$ of that feature,\n",
    "        split the dataset according to the condition \"x_m <= s\" (self.split_data(...)).\n",
    "        The split \"x_m <= s\" generates two datatsets dataset_left and dataset_right.\n",
    "        Compute the information gain associated to the three datasets (self.information_gain).\n",
    "        Select the split associated with the largest information gain.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset before split.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        best_split : dict\n",
    "            dictionary to store the best split.\n",
    "            The keys of the dictionare are: \n",
    "            \"feature_index\", \"threshold\", \"dataset_left\", \"dataset_right\", \"gain\".\n",
    "        \"\"\"\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over features\n",
    "        nb_features = dataset.shape[1] - 1\n",
    "        for m in range(nb_features):\n",
    "            \n",
    "            thresholds = dataset[:, m]\n",
    "            thresholds = np.unique(thresholds)\n",
    "            \n",
    "            # loop over thresholds\n",
    "            for s in thresholds:\n",
    "                \n",
    "                # datasets assoociated to split \"x_m <= s\"\n",
    "                split = (m, s)\n",
    "                dataset_left, dataset_right = self.split_data(dataset, split)\n",
    "                \n",
    "                # check if datasets are not empty\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    \n",
    "                    # compute information gain\n",
    "                    gain = self.information_gain(dataset, dataset_left, dataset_right)\n",
    "                    \n",
    "                    # update the best split if needed\n",
    "                    if gain > max_gain:\n",
    "                        \n",
    "                        best_split[\"feature_index\"] = m\n",
    "                        best_split[\"threshold\"] = s\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"gain\"] = gain\n",
    "                        max_gain = gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    \n",
    "    def leaf_value(self, region):\n",
    "        \"\"\"\n",
    "        Compute the most frequent target of a non-splitable region. \n",
    "        The region is associated to a leaf node of the decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : ndarray\n",
    "            Non-splitable dataset that corresponds to a region oof the partition.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        most_frequent : float\n",
    "            Most frequent target of the data in the region.\n",
    "        \"\"\"\n",
    "        \n",
    "        Y = region[:, -1]        \n",
    "        Y = list(Y)\n",
    "        most_frequent = max(Y, key=Y.count)\n",
    "        \n",
    "        return most_frequent\n",
    "    \n",
    "    \n",
    "    def build_tree(self, dataset, depth=0):\n",
    "        \"\"\"\n",
    "        Builds the decision tree recursively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Initial dataset to be split by the decision tree.\n",
    "        depth : int\n",
    "            Depth of the decision tree that has been built so far.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DecisionNode : DecisionNode\n",
    "            The decision tree that represent the best successive splits\n",
    "            for to the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_samples = np.shape(dataset)[0]\n",
    "                \n",
    "        # compute decision node\n",
    "        if num_samples >= self.min_samples and depth <= self.max_depth:\n",
    "            \n",
    "            # find the best split\n",
    "            split = self.best_split(dataset)\n",
    "            \n",
    "            # check if vinformation gain is positive\n",
    "            if split[\"gain\"] >= 0:\n",
    "                \n",
    "                # recursive call left\n",
    "                subtree_left = self.build_tree(split[\"dataset_left\"], depth + 1)\n",
    "                \n",
    "                # recursive call right\n",
    "                subtree_right = self.build_tree(split[\"dataset_right\"], depth + 1)\n",
    "                \n",
    "                return DecisionNode(split[\"feature_index\"], split[\"threshold\"], \n",
    "                                    subtree_left, subtree_right, split[\"gain\"])\n",
    "            \n",
    "        # compute leaf node\n",
    "        else:\n",
    "            \n",
    "            value = self.leaf_value(dataset)\n",
    "            \n",
    "            return DecisionNode(value=value)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fits the decision tree on the features X and targets Y.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Feature columns of the dataset.\n",
    "        Y : ndarray\n",
    "            Target column of the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "        \n",
    "    def predict_single_point(self, x, tree):\n",
    "        \"\"\"        \n",
    "        Predict the target y_hat associated to a point x.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Point x = (x_1,...,x_M) whose target is to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_hat : float\n",
    "            Target associated to x.\n",
    "        \"\"\"\n",
    "        \n",
    "        if tree.value != None:\n",
    "            \n",
    "            return tree.value\n",
    "        \n",
    "        # x_m <= s\n",
    "        if x[tree.feature_index] <= tree.threshold:\n",
    "            \n",
    "            return self.predict_single_point(x, tree.left)\n",
    "        \n",
    "        # x_m > s\n",
    "        else:\n",
    "            \n",
    "            return self.predict_single_point(x, tree.right)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the targets associated to a set of points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Tensor of points X (dim N x M) whose targets are to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Y : ndarray\n",
    "            Tensor of targets (dim N x 1) associated to X.\n",
    "        \"\"\"\n",
    "        \n",
    "        preditions = np.array([self.predict_single_point(x, self.root) for x in X])\n",
    "        \n",
    "        return preditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76749949",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "Complétez la méthode `split_data(...)` qui, étant donné un dataset `dataset` et un split `split` de la forme $x_m \\leq s$, retourne deux datasets qui correspondent aux lignes de `dataset` qui satisfont et ne satisfont pas la condition `split`, respectivement.\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "split = (2, 0.1) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "dataset_left.shape, dataset_right.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11d6346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 5), (150, 5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "split = (2, 0.1) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "dataset_left.shape, dataset_right.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338986fa",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "Complétez la méthode `information_gain(...)` qui, étant donné trois datasets `dataset`, `dataset_left` et `dataset_right`, retourne le **gain informationnel** associé aux targets $y_i$ de `dataset` et de `dataset_left` et `dataset_right`. \n",
    "\n",
    "Ici, le gain informationnel correspond à la **réduction d'entropie**. Le calcul de l'entropie d'un ensemble de targets `y` est obtenu par la méthode `entropy(...)` déjà implémentée. Le **gain informationnel** est alors calculé comme suit:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "entropy   & = & \\mathrm{Entropy} \\left\\{ y_i : i \\in \\mathrm{dataset} \\right\\} \\\\\n",
    "&& \\\\\n",
    "entropy_l & = & \\frac{|\\mathrm{dataset\\_left}|}{|\\mathrm{dataset}|} \\cdot \\mathrm{Entropy} \\left\\{ y_i : i \\in \\mathrm{dataset\\_left} \\right\\} \\\\\n",
    "&& \\\\\n",
    "entropy_r & = & \\frac{|\\mathrm{dataset\\_right}|}{|\\mathrm{dataset}|} \\cdot \\mathrm{Entropy} \\left\\{ y_i : i \\in \\mathrm{dataset\\_right} \\right\\} \\\\\n",
    "&& \\\\\n",
    "gain & = & entropy - (entropy_l + entropy_r)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "où $|X|$ dénote le nombre d'éléments de l'ensemble $X$.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "split = (2, 1.5) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "tree.information_gain(dataset, dataset_left, dataset_right)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c4544d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303699177904257"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "split = (2, 1.5) # split x_2 <= 0.1\n",
    "dataset_left, dataset_right = tree.split_data(dataset, split)\n",
    "tree.information_gain(dataset, dataset_left, dataset_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10830a27",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "\n",
    "Complétez la méthode `best_split(...)` qui, étant donné un dataset `dataset`, retourne le split induisant le plus grand gain informationnel. Ce best split est obtenu par l'algorithme suivant:\n",
    "\n",
    "<img src=\"files/figures/DT_algo_3.png\" width=\"650px\"/>\n",
    "\n",
    "La variable `best_split` sera un dictionnaire avec les clés suivantes:\n",
    "```\n",
    "\"feature_index\"\n",
    "\"threshold\"\n",
    "\"dataset_left\"\n",
    "\"dataset_right\"\n",
    "\"gain\"\n",
    "```\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "819b6632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1.9, 0.9182958340544894)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "best_split[\"feature_index\"], best_split[\"threshold\"], best_split[\"gain\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79d905",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "\n",
    "Complétez la méthode `leaf_value(...)` qui, étant donné un sous-ensemble du dataset `region`, retourne la target qui apparaît le plus fréquemment dans ce sous-dataset. On rappelle que chaque feuille de l'arbre de décision est associée avec une région non-splitable du dataset.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "region = best_split['dataset_right']\n",
    "tree.leaf_value(region)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc5a92ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Versicolor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "dataset = np.array(data)\n",
    "best_split = tree.best_split(dataset)\n",
    "region = best_split['dataset_right']\n",
    "tree.leaf_value(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e378d64",
   "metadata": {},
   "source": [
    "### Exercice 5\n",
    "\n",
    "Complétez la méthode **récursive** `build_tree(...)` qui, étant donné un dataset `dataset` et une profondeur `depth`, retourne l'arbre de décision correspondant à ce dataset. L'arbre de décision est donné par l'algorithme **recursive binary plitting** ci-dessous.\n",
    "\n",
    "<img src=\"files/figures/DT_algo_4.png\" width=\"650px\"/>\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "dataset = np.array(data)\n",
    "tree = DecisionTreeClassifier(min_samples=3, max_depth=3)\n",
    "tree.root = tree.build_tree(dataset)\n",
    "tree.root.print_tree()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe079b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Setosa\n",
      "\t X_0 < 4.30\n",
      "\t\t\t\t Setosa\n",
      "\t\t\t X_1 < 2.90\n",
      "\t\t\t\t Setosa\n",
      "\t\t X_0 < 4.40\n",
      "\t\t\t\t Setosa\n",
      "\t\t\t X_0 < 4.50\n",
      "\t\t\t\t Setosa\n",
      " X_2 < 1.90\n",
      "\t\t\t\t Versicolor\n",
      "\t\t\t X_3 < 1.60\n",
      "\t\t\t\t Virginica\n",
      "\t\t X_2 < 4.90\n",
      "\t\t\t\t Virginica\n",
      "\t\t\t X_3 < 1.50\n",
      "\t\t\t\t Versicolor\n",
      "\t X_3 < 1.70\n",
      "\t\t\t\t Versicolor\n",
      "\t\t\t X_0 < 5.90\n",
      "\t\t\t\t Virginica\n",
      "\t\t X_2 < 4.80\n",
      "\t\t\t\t Virginica\n",
      "\t\t\t X_0 < 5.60\n",
      "\t\t\t\t Virginica\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array(data)\n",
    "tree = DecisionTreeClassifier(min_samples=3, max_depth=3)\n",
    "tree.root = tree.build_tree(dataset)\n",
    "tree.root.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af63a76",
   "metadata": {},
   "source": [
    "### Exercice 6\n",
    "\n",
    "Complétez la méthode `fit(...)` qui correspond à l'entaînement de l'arbre de décision sur les features `X` et les targets `Y` du dataset. Cette méthode contruit l'arbre de décision associée au dataset $(X, Y)$ et assigne ce dernier comme racine de l'arbre.\n",
    "\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples=3, max_depth=3)\n",
    "classifier.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "classifier.root.print_tree()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9afb94",
   "metadata": {},
   "source": [
    "### Exercice 7\n",
    "\n",
    "Complétez la méthode **récursive** `predict_single_point(...)` qui retourne la prédiction `y_hat` associée au point `x`. Cette méthode fait passer `x` dans les noeuds sucessifs de l'arbre de décision `tree` jusqu'à atteindre une feuille dont la valeur sera la prédiction.\n",
    "\n",
    "Complétez ensuite la méthode `predict(...)` qui retourne l'enemble des prédictions `X` associée à un ensemble de points `X` (tenseur). Cette méthode appelle la précédent sur chaque ligne de `X`.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples=3, max_depth=3)\n",
    "classifier.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "classifier.predict_single_point(np.array[0.1, 0.2, 0.3, 0.4. 0.5], classifier.root)\n",
    "classifier.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fe3bcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Versicolor', 'Setosa', 'Virginica', 'Versicolor', 'Versicolor',\n",
       "       'Setosa', 'Versicolor', 'Virginica', 'Versicolor', 'Versicolor',\n",
       "       'Virginica', 'Setosa', 'Setosa', 'Setosa', 'Setosa', 'Versicolor',\n",
       "       'Virginica', 'Versicolor', 'Versicolor', 'Virginica', 'Setosa',\n",
       "       'Virginica', 'Setosa', 'Virginica', 'Virginica', 'Virginica',\n",
       "       'Virginica', 'Virginica', 'Setosa', 'Setosa'], dtype='<U10')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "classifier = DecisionTreeClassifier(min_samples=3, max_depth=3)\n",
    "classifier.fit(X_train, Y_train)   # entraînement sur le train set\n",
    "classifier.predict_single_point(np.array([0.1, 0.2, 0.3, 0.4, 0.5]), classifier.root)\n",
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3cc43",
   "metadata": {},
   "source": [
    "### Exercice 8\n",
    "\n",
    "Exécutez le code ci-dessous pour voir si votre implémentation de la classe ``DecisionTreeClassifier`` fonctionne coorrectement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456c3a",
   "metadata": {},
   "source": [
    "#### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b310c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744240d",
   "metadata": {},
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aafa670",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples=3, max_depth=3)\n",
    "classifier.fit(X_train,Y_train)\n",
    "# classifier.root.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fe091",
   "metadata": {},
   "source": [
    "#### Résultats sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8119214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        10\n",
      "  Versicolor       1.00      1.00      1.00         9\n",
      "   Virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = classifier.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd6a51",
   "metadata": {},
   "source": [
    "**Résultats parfaits!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
