{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b3d60e",
   "metadata": {},
   "source": [
    "# Forêts aléatoires (random forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1838c04",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/RF.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d670aa3",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d80fb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159601ec",
   "metadata": {},
   "source": [
    "## Loader les data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae9c1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/airfoil_self_noise.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aa869341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>126.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x0   x1      x2    x3        x4        y\n",
       "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
       "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
       "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
       "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
       "4  2000  0.0  0.3048  71.3  0.002663  127.461"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e055f",
   "metadata": {},
   "source": [
    "Nous allons prédire la variable `y` en fonction des variables `x0`, `x1`, `x2`, `x3`, `x4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bd645",
   "metadata": {},
   "source": [
    "## Forêt aléatoire dans un contexte de régression\n",
    "\n",
    "- Une **forêt aléatoire (random forest)** est un **modèle ensembliste** obtenu à partir de plusieurs **arbres de décision (decision trees)**.\n",
    "- On utilise une technique de **bagging** améliorée pour agréger et décorréler les arbres de décision utilisés.\n",
    "- La technique de décorrélation des arbres de décision a pour but de diminuer la variance du modèle final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a819e",
   "metadata": {},
   "source": [
    "La classe `DecisionNode` ci-dessous représente un **noeud** d'un **arbre de décision**, ou encore, un **arbre de décision** en soi. \n",
    "\n",
    "*En effet, en programmation, un objet \"noeud\" qui pointe vers ses fils est une structure de données suffisante pour représenter un arbre binaire.*\n",
    "\n",
    "- Un **noeud interne** représente un *split* de la forme $x_m \\leq s$ (où $x_m$ est la $m$-ième composante de $\\mathbf{x}$).<br>\n",
    "Chaque noeud interne donc est associé à un `feature_index` $m$ et un `threshold` $s$.<br>\n",
    "Un noeud interne est également associé à `var_red`: la réduction de variance engendrée par son split.\n",
    "\n",
    "\n",
    "- Une **feuille** représente un sous-ensemble des data $R_k$ (*région*, *cellule de partition*).<br>\n",
    "Chaque feuille est donc associée une `value`: la target moyenne des data dans $R_m$<br>\n",
    "\n",
    "$$\\hat{y}_{R_k} = \\frac{1}{|R_k|} \\sum_{\\{i : \\mathbf{x_i} \\in R_k\\}} y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f30a9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"\n",
    "    Implements a decision node, or quivalently, a decision tree.\n",
    "    \n",
    "    As usual, a binary tree is identified with a root node contaning as chidren\n",
    "    a left subtree and a right subtree.\n",
    "    \n",
    "    Here, an *internal node* represents a split of the form \"x_m <= s\",\n",
    "    where m is the `feature_index` and s the `thresold` of the split.\n",
    "    \n",
    "    A *leaf node* represents a subset of the data (region, box, partition cell).\n",
    "    It is associated with a `value`: the average target for the data in this region.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_index=None, threshold=None, \n",
    "                 left=None, right=None, var_red=None, value=None):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_index : int\n",
    "            index m of variable x_m for the node split (x_m <= s).\n",
    "        threshold : Union[int, float]\n",
    "            threshold s for the node split (x_m <= s).\n",
    "        left : Union[DecisionNode, None]\n",
    "            Left child of the node\n",
    "        right : Union[DecisionNode, None]\n",
    "            Right child of the node\n",
    "        var_red : float\n",
    "            variance reduction induced by the node split (x_m <= s).\n",
    "        value : Union[float, None]\n",
    "            if the node is a leaf, then value of this leaf.\n",
    "        \"\"\"\n",
    "\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.var_red = var_red\n",
    "\n",
    "        # for leaf node\n",
    "        self.value = value\n",
    "        \n",
    "        # children\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "        \n",
    "    def print_tree(self, tab=0):\n",
    "        \"\"\"Prints the node (i.e., the tree).\"\"\"\n",
    "        \n",
    "        if self.left is None and self.right is None: # leaf\n",
    "            \n",
    "            print(\"\\t\"*(tab), f\"{self.value:.2f}\")\n",
    "            \n",
    "        else:                                        # internal node\n",
    "            self.left.print_tree(tab+1)\n",
    "            print(\"\\t\"*(tab), f\"X_{self.feature_index} < {self.threshold:.2f}\")\n",
    "            self.right.print_tree(tab+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a5510",
   "metadata": {},
   "source": [
    "La classe `DecisionTreeRegressor` ci-dessous représente un **arbre de décision** dans un contexte de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6fe70cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    \"\"\"\n",
    "    Implements a decision tree for regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_samples : int\n",
    "            Minimal number of data necessary to envision a split.\n",
    "            Otherwise, the node is a leaf (subset of data).\n",
    "        max_depth : int\n",
    "            Maximal depth of the tree.\n",
    "            When max_depth is reached, no further split is performed.\n",
    "        \"\"\"\n",
    "        \n",
    "        # the root will become the decision tree\n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    \n",
    "    def split_data(self, dataset, split):\n",
    "        \"\"\"\n",
    "        Splits dataset according to a given split of the form x_m <= s.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset to be split\n",
    "        split : tuple\n",
    "            A split \"x_m <= s\" is represented by the tuple (m, s),\n",
    "            where m is the feature_index and s the threshold.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dataset_left, dataset_right : tuple[ndarray, ndarray]\n",
    "            dataset_left: data satisfying the condition x_m <= s.\n",
    "            dataset_right: data satisfying the scondition x_m > s.\n",
    "        \"\"\"\n",
    "        \n",
    "        feature_index, threshold = split\n",
    "        mask = dataset[:, feature_index] <= threshold\n",
    "        dataset_left = dataset[mask, :]\n",
    "        dataset_right = dataset[np.logical_not(mask), :]\n",
    "        \n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    \n",
    "    def variance_reduction(self, dataset, dataset_left, dataset_right):\n",
    "        \"\"\"\n",
    "        Computes the variance reduction of the targets induced by a split.\n",
    "        \n",
    "        Suppose that a split of a dataset has induces the 2 datasets:\n",
    "        dataset_left and dataset_right (cf. method split_data).\n",
    "        This function computes the variance $var_1$ of the y's of dataset as well as\n",
    "        the weighted variance $var_2$ of the y's of left_dataset and right_dataset.\n",
    "        The variance reduction is then given by $var_1 - var_2$.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset before split.\n",
    "        dataset_left : ndarray\n",
    "            First dataset induced by the split.\n",
    "        dataset_right : ndarray\n",
    "            Second dataset induced by the split.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        var_reduction : float\n",
    "            variance reduction induced by the split.\n",
    "        \"\"\"\n",
    "        \n",
    "        y = dataset[:, -1]\n",
    "        y_l = dataset_left[:, -1]\n",
    "        y_r = dataset_right[:, -1]\n",
    "        \n",
    "        w_l = len(y_l) / len(y)\n",
    "        w_r = len(y_r) / len(y)\n",
    "        \n",
    "        var_reduction = np.var(y) - ( w_l * np.var(y_l) + w_r * np.var(y_r) )\n",
    "        \n",
    "        return var_reduction\n",
    "\n",
    "    \n",
    "    def best_split(self, dataset):\n",
    "        \"\"\"\n",
    "        Computes the best split for a dataset.\n",
    "        \n",
    "        For all feature $m$ and and all possible value $s$ of that feature,\n",
    "        split the dataset according to the condition \"x_m <= s\" (self.split_data(...)).\n",
    "        The split \"x_m <= s\" generates two datatsets: dataset_left and dataset_right.\n",
    "        Compute the variance reduction associated to the three datasets (self.variance_reduction).\n",
    "        Select the split associated with the largest variance reduction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Dataset before split.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        best_split : dict\n",
    "            dictionary to store the best split.\n",
    "            The keys of the dictionare are: \n",
    "            \"feature_index\", \"threshold\", \"dataset_left\", \"dataset_right\", \"var_red\".\n",
    "        \"\"\"\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        best_var_red = -float(\"inf\")\n",
    "        \n",
    "        # loop over features\n",
    "        nb_features = dataset.shape[1] - 1\n",
    "        for m in range(nb_features):\n",
    "            \n",
    "            thresholds = dataset[:, m]\n",
    "            thresholds = np.unique(thresholds)\n",
    "            \n",
    "            # loop over thresholds\n",
    "            for s in thresholds:\n",
    "                \n",
    "                # datasets assoociated to split \"x_m <= s\"\n",
    "                split = (m, s)\n",
    "                dataset_left, dataset_right = self.split_data(dataset, split)\n",
    "                \n",
    "                # check if datasets are not empty\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    \n",
    "                    # compute variance reduction\n",
    "                    var_red = self.variance_reduction(dataset, dataset_left, dataset_right)\n",
    "                    \n",
    "                    # update the best split if needed\n",
    "                    if var_red > best_var_red:\n",
    "                        \n",
    "                        best_split[\"feature_index\"] = m\n",
    "                        best_split[\"threshold\"] = s\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"var_reduction\"] = var_red\n",
    "                        best_var_red = var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    \n",
    "    def leaf_value(self, region):\n",
    "        \"\"\"\n",
    "        Compute the mean of targets of a non-splitable region. \n",
    "        Teh region is associated to a leaf node of the decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : ndarray\n",
    "            Non-splitable dataset that corresponds to a region oof the partition.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mean : float\n",
    "            Mean of targets for the data in the region.\n",
    "        \"\"\"\n",
    "        \n",
    "        Y = region[:, -1]\n",
    "        mean = np.mean(Y)\n",
    "        \n",
    "        return mean\n",
    "    \n",
    "    \n",
    "    def build_tree(self, dataset, depth=0):\n",
    "        \"\"\"\n",
    "        Builds the decision tree recursively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Initial dataset to be split by the decision tree.\n",
    "        depth : int\n",
    "            Depth of the decision tree that has been built so far.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DecisionNode : DecisionNode\n",
    "            The decision tree that represent the best successive splits\n",
    "            for to the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_samples = np.shape(dataset)[0]\n",
    "                \n",
    "        # compute decision node\n",
    "        if num_samples >= self.min_samples and depth <= self.max_depth:\n",
    "            \n",
    "            # find the best split\n",
    "            split = self.best_split(dataset)\n",
    "            \n",
    "            # check if variance reduction is positive\n",
    "            if split[\"var_reduction\"] >= 0:\n",
    "                \n",
    "                # recursive call left\n",
    "                subtree_left = self.build_tree(split[\"dataset_left\"], depth + 1)\n",
    "                \n",
    "                # recursive call right\n",
    "                subtree_right = self.build_tree(split[\"dataset_right\"], depth + 1)\n",
    "                \n",
    "                return DecisionNode(split[\"feature_index\"], split[\"threshold\"], \n",
    "                                    subtree_left, subtree_right, split[\"var_reduction\"])\n",
    "            \n",
    "        # compute leaf node\n",
    "        else:\n",
    "            \n",
    "            value = self.leaf_value(dataset)\n",
    "            \n",
    "            return DecisionNode(value=value)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the decision tree on the features X and targets y.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Feature columns of the dataset.\n",
    "        y : ndarray\n",
    "            Target column of the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset = np.concatenate((X, y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "        \n",
    "    def predict_single_point(self, x, tree):\n",
    "        \"\"\"        \n",
    "        Predict the target y_hat associated to a point x.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Point x = (x_1,...,x_M) whose target is to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_hat : float\n",
    "            Target associated to x.\n",
    "        \"\"\"\n",
    "        \n",
    "        if tree.value != None:\n",
    "            \n",
    "            return tree.value\n",
    "        \n",
    "        # x_m <= s\n",
    "        if x[tree.feature_index] <= tree.threshold:\n",
    "            \n",
    "            return self.predict_single_point(x, tree.left)\n",
    "        \n",
    "        # x_m > s\n",
    "        else:\n",
    "            \n",
    "            return self.predict_single_point(x, tree.right)\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the targets associated to a set of points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Tensor of points X (dim N x M) whose targets are to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predictions : ndarray\n",
    "            Tensor of targets (dim N x 1) associated to X.\n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = np.array([self.predict_single_point(x, self.root) for x in X])\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f53cc",
   "metadata": {},
   "source": [
    "La classe `RandomForest` ci-dessous représente une **forêt aléatoire** dans un contexte de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e2ccb1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"\n",
    "    Implements a random forest for regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_trees=10, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        nb_trees : int\n",
    "            Number of decision trees in the random forest.\n",
    "        min_samples : int\n",
    "            When building a decision tree in the forest,\n",
    "            minimal number of data necessary to envision a split.\n",
    "            Otherwise, the node is a leaf (subset of data).\n",
    "        max_depth : int\n",
    "            When building a decision tree in the forest,\n",
    "            maximal depth of the tree.\n",
    "            When max_depth is reached, no further split is performed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_trees = nb_trees\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.decision_trees = []\n",
    "    \n",
    "    \n",
    "    def generate_sampled_dataset(self, X, y, n=None):\n",
    "        \"\"\"\n",
    "        Generate a new \"bootstrapped\" dataset by drawing \n",
    "        n samples with replacemnt from the dataset (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Size of the bootstrapped dataset\n",
    "        X : ndarray\n",
    "            Feature columns of the dataset.\n",
    "        y : ndarray\n",
    "            Target column of the dataset.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_new : ndarray\n",
    "            Bootstrapped tensor of features (n x nb_features).\n",
    "        y_new : ndarray\n",
    "            Bootstrapped tensor of targets (n x 1).\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the random forest on the features X and targets y.\n",
    "        The successive decision trees composing the forest are stored in the list self.decision_trees.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Feature columns of the dataset.\n",
    "        y : ndarray\n",
    "            Target column of the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the targets associated to a set of points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Tensor of points X whose targets are to be predicted.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predictions : ndarray\n",
    "            Tensor of targets associated to X.\n",
    "        \"\"\"\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76749949",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "Complétez la méthode `generate_sampled_dataset(...)` de la classe `RandomForest` qui, étant donné un dataset `X, y` et un entier `n`, retourne un nouveau dataset `X_new, y_new` qui corrrespondant à `n` tirages aléatoires avec remplacement d'éléments de `X, y`.\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "rf = RandomForest()\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "X_new , y_new = rf.generate_sampled_dataset(X_train, y_train, n=10)\n",
    "X_new.shape, y_new.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10830a27",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "Complétez la méthode `fit(...)` de la classe `RandomForest` qui, étant donné un dataset `X, y`, retourne une liste de $B$ arbres de décision (où $B$ correspond au paramètre `nb_trees`) comme  décrit dans l'algorithme ci-desous:\n",
    "- Ne pas implémenter la méthode `aggregate`, mais retourner simplement la liste des arbres de décision;\n",
    "- Les arbres de decisions qui composent la forêt aléatoire sont stockés dans l'attribut `self.decision_trees`.\n",
    "- Le nombre d'arbres aléatoires qui composent la forêt est `self.nb_trees`;\n",
    "\n",
    "<img src=\"files/figures/RF_algo.png\" width=\"430px\"/>\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "rf = RandomForest()\n",
    "rf.fit(X_train, y_train)\n",
    "len(rf.decision_trees)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b6632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf79d905",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "\n",
    "Complétez la méthode `predict(...)` qui retourne les prédictions `y_test_preds` de la forêt aléatoire sur un ensemble de points `X_test`. On rappelle que la prédiction $\\hat{y}_i = \\hat{f}_{bag}(\\mathbf{x_i})$ de la forêt aléatoire correspond à la moyenne des prédictions $\\hat{f}_{b}(\\mathbf{x_i})$ de tous les arbres qui composent la forêt, pour $b=1,\\dots,B$.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "y_test_preds = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_test_preds))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a92ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97c1adc",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "\n",
    "Exécutez le code ci-dessous pour voir si votre implémentation de la classe ``RandomForest`` fonctionne coorrectement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456c3a",
   "metadata": {},
   "source": [
    "#### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b310c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744240d",
   "metadata": {},
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aafa670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest composed of 100 decision trees\n",
    "rf = RandomForest(min_samples=3, max_depth=10, nb_trees=100)\n",
    "rf.fit(X_train, y_train)\n",
    "len(rf.decision_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fe091",
   "metadata": {},
   "source": [
    "#### Résultats sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8119214",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds = rf.predict(X_test)\n",
    "print(\"MSE:\", np.sqrt(mean_squared_error(y_test, y_test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0155dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(y_test, y_test_preds)\n",
    "plt.xlabel(\"Test targets\")\n",
    "plt.ylabel(\"Test predictions\")\n",
    "plt.title(\"Targets vs Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78e515",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "- Dans le cas d'un arbre de décision, on avait une MSE de ~2.60, La forêt aléatoire améliore donc le résultat!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
